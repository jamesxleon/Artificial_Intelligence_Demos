{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "### PDF to JSON approach (towards converting the pdfs to an alpaca-like dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\n]+', ' ', text)\n",
    "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def pdf_to_txt(pdf_file_path):\n",
    "    with pdfplumber.open(pdf_file_path) as pdf:\n",
    "        extracted_text = ''\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            page_text = clean_text(page_text)\n",
    "            extracted_text += ' ' + page_text\n",
    "    return extracted_text\n",
    "\n",
    "pdf_file_path = \"PDFs/Tests/BRIEFING CUD 2023.pdf\"\n",
    "extracted_text = pdf_to_txt(pdf_file_path)\n",
    "print(extracted_text[:1000])  # Print the first 1000 characters of the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After cleaning the resulting text and adding labels 'Instruction:', 'Input:' and 'Output:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def extract_info_from_text(file_path):\n",
    "    # Open the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the file content\n",
    "        content = file.read()\n",
    "\n",
    "    # Create a pattern for instructions, inputs, and outputs\n",
    "    pattern = r'Instruction:(.*?)Input:(.*?)Output:(.*?)(?=Instruction:|$)'\n",
    "    \n",
    "    # Find all matches in the file content\n",
    "    matches = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    # Initialize an empty list to store the information\n",
    "    info_list = []\n",
    "    \n",
    "    # Loop over each match\n",
    "    for match in matches:\n",
    "        # Get the instruction, input, and output\n",
    "        instruction = match[0].strip()\n",
    "        input_ = match[1].strip()\n",
    "        output = match[2].strip()\n",
    "        \n",
    "        # Add the information to the list\n",
    "        info_list.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_,\n",
    "            \"output\": output\n",
    "        })\n",
    "\n",
    "    return info_list\n",
    "\n",
    "# Path to the text file\n",
    "text_file_path = \"Texts/Tests/1.txt\"\n",
    "\n",
    "# Extract the information from the text file\n",
    "info_list = extract_info_from_text(text_file_path)\n",
    "\n",
    "# Write the information to a JSON file\n",
    "with open(\"JSONs/Tests/output.json\", 'w') as json_file:\n",
    "    json.dump(info_list, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the file and read its content\n",
    "with open('Texts/Tests/4.txt', 'r', encoding='utf-8') as file:\n",
    "    file_content = file.readlines()\n",
    "\n",
    "# Print the first few lines of the file to understand its structure\n",
    "file_content[:10]\n",
    "\n",
    "# Initialize empty list to hold our data dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Initialize empty dictionary to hold individual data\n",
    "data_dict = {'instruction': '', 'input': '', 'output': ''}\n",
    "\n",
    "# Create a variable to hold the current section we're parsing\n",
    "current_section = ''\n",
    "\n",
    "# Iterate through each line in the file content\n",
    "for line in file_content:\n",
    "    line = line.strip()  # Remove any leading/trailing whitespaces\n",
    "\n",
    "    # Depending on the type of line, update the data dictionary\n",
    "    if line.startswith('Instruction:'):\n",
    "        current_section = 'instruction'\n",
    "        # If we are starting a new 'instruction', and our data_dict is not empty, append it to the data_list\n",
    "        # The check for emptiness ensures we don't append an empty dictionary at the start\n",
    "        if data_dict['instruction']:\n",
    "            data_list.append(data_dict)\n",
    "            # Start a new data_dict for the new 'instruction'\n",
    "            data_dict = {'instruction': '', 'input': '', 'output': ''}\n",
    "\n",
    "    elif line.startswith('Input:'):\n",
    "        current_section = 'input'\n",
    "\n",
    "    elif line.startswith('Output:'):\n",
    "        current_section = 'output'\n",
    "\n",
    "    elif line:  # If line is not empty\n",
    "        # Append the line to the current section in the dictionary\n",
    "        data_dict[current_section] += (line + '\\n') if data_dict[current_section] else line\n",
    "\n",
    "# Append the last dictionary to the list\n",
    "if data_dict['instruction']:\n",
    "    data_list.append(data_dict)\n",
    "\n",
    "# Convert the list of dictionaries to a JSON string\n",
    "json_data = json.dumps(data_list, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Write the JSON data to a file\n",
    "with open('JSONs/Tests/4.json', 'w', encoding='utf-8') as file:\n",
    "    file.write(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data from the file\n",
    "with open('JSONs/Tests/4.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Iterate through the data and replace '\\n' in the strings\n",
    "for dictionary in data:\n",
    "    for key, value in dictionary.items():\n",
    "        dictionary[key] = value.replace('\\n', ' ')\n",
    "\n",
    "# Write the cleaned data back to a JSON file\n",
    "with open('JSONs/Tests/DB_inicial_3.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 JSON files.\n",
      "Processing file: DB_inicial_2.json\n",
      "Processing file: DB_inicial.json\n",
      "Processing file: DB_inicial_3.json\n",
      "Processing file: DB_inicial_1.json\n",
      "Merging complete. The merged data is saved in merged.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def merge_json_files(directory):\n",
    "    # Initialize an empty list to store all data\n",
    "    all_data = []\n",
    "\n",
    "    # Get a list of all JSON files in the directory\n",
    "    json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
    "\n",
    "    # Check if there are any JSON files in the directory\n",
    "    if not json_files:\n",
    "        print('No JSON files found in the directory.')\n",
    "        return\n",
    "\n",
    "    print(f'Found {len(json_files)} JSON files.')\n",
    "\n",
    "    # Loop through all JSON files\n",
    "    for file_name in json_files:\n",
    "        print(f'Processing file: {file_name}')\n",
    "        # Open each JSON file\n",
    "        with open(os.path.join(directory, file_name)) as file:\n",
    "            # Load the data from the JSON file\n",
    "            data = json.load(file)\n",
    "            # Add the data to the list\n",
    "            all_data.extend(data)\n",
    "\n",
    "    # Save the combined data to a new JSON file, set ensure_ascii=False to preserve accents\n",
    "    with open('merged.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(all_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print('Merging complete. The merged data is saved in merged.json')\n",
    "\n",
    "# Specify the directory that contains the JSON files\n",
    "directory = 'JSONs/Tests'\n",
    "\n",
    "# Call the function to merge the JSON files\n",
    "merge_json_files(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join txts together\n",
    "def merge_txt_files(directory, output_file):\n",
    "    # Get a list of all txt files in the directory\n",
    "    txt_files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "\n",
    "    # Check if there are any txt files in the directory\n",
    "    if not txt_files:\n",
    "        print('No txt files found in the directory.')\n",
    "        return\n",
    "\n",
    "    print(f'Found {len(txt_files)} txt files.')\n",
    "\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for file_name in txt_files:\n",
    "            print(f'Processing file: {file_name}')\n",
    "            with open(os.path.join(directory, file_name)) as infile:\n",
    "                # write file content to outfile\n",
    "                outfile.write(infile.read())\n",
    "                # write a line break\n",
    "                outfile.write('\\n')\n",
    "\n",
    "    print('Merging complete. The merged data is saved in', output_file)\n",
    "\n",
    "# Specify the directory that contains the txt files\n",
    "directory = 'Texts/Nontests'\n",
    "# Specify the name of the output file\n",
    "output_file = 'merged.txt'\n",
    "\n",
    "# Call the function to merge the txt files\n",
    "merge_txt_files(directory, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF to Text approach (raw text training dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove any non-alphanumeric characters, but keep line breaks\n",
    "    text = re.sub(r'[^\\w\\n]+', ' ', text)\n",
    "\n",
    "    # Replace multiple spaces with a single space, but keep line breaks\n",
    "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def pdf_to_txt(pdf_file_path, txt_file_path):\n",
    "    # Open the PDF file\n",
    "    with pdfplumber.open(pdf_file_path) as pdf:\n",
    "        # Initialize an empty string to store the extracted text\n",
    "        extracted_text = ''\n",
    "\n",
    "        # Loop over each page in the PDF\n",
    "        for page in pdf.pages:\n",
    "            # Extract the text from the page\n",
    "            page_text = page.extract_text()\n",
    "\n",
    "            # Clean the extracted text\n",
    "            page_text = clean_text(page_text)\n",
    "\n",
    "            # Append the page text to the extracted text\n",
    "            extracted_text += ' ' + page_text\n",
    "\n",
    "    # Write the extracted text to a text file\n",
    "    with open(txt_file_path, 'w') as txt_file:\n",
    "        txt_file.write(extracted_text)\n",
    "\n",
    "# Directory containing the PDF files\n",
    "pdf_directory = \"PDFs/Nontests\"\n",
    "\n",
    "# Directory to store the text files\n",
    "txt_directory = \"Texts/Nontests\"\n",
    "\n",
    "# Ensure the text directory exists\n",
    "os.makedirs(txt_directory, exist_ok=True)\n",
    "\n",
    "# Loop over each file in the PDF directory\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    # Check if the file is a PDF\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        # Construct the full file path\n",
    "        pdf_file_path = os.path.join(pdf_directory, filename)\n",
    "\n",
    "        # Construct the text file path\n",
    "        txt_file_path = os.path.join(txt_directory, filename.replace(\".pdf\", \".txt\"))\n",
    "\n",
    "        # Convert the PDF to text\n",
    "        pdf_to_txt(pdf_file_path, txt_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discarded approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pdfreader import SimplePDFViewer\n",
    "\n",
    "#This function does the work but the result is difficult to tokenize\n",
    "#def convert_pdf_to_txt(file_path):\n",
    "    # Open the PDF file in read-binary mode\n",
    "#    with open(file_path, 'rb') as file:\n",
    "        # Create a PDF file viewer object\n",
    "#        viewer = SimplePDFViewer(file)\n",
    "\n",
    "        # Initialize an empty string to hold the extracted text\n",
    "#        text = ''\n",
    "\n",
    "        # Loop through each page in the PDF and extract the text\n",
    "#       for canvas in viewer:\n",
    "#            viewer.render()\n",
    "#            text += ''.join(viewer.canvas.strings)\n",
    "\n",
    "    # Create the Texts directory if it doesn't exist\n",
    "#    if not os.path.exists('Texts'):\n",
    "#        os.makedirs('Texts')\n",
    "\n",
    "    # Write the extracted text to a .txt file in the Texts directory\n",
    "#    with open('Texts/output1.txt', 'w') as output_file:\n",
    "#        output_file.write(text)\n",
    "\n",
    "# Call the function with the path to your PDF file\n",
    "#convert_pdf_to_txt(\"PDFs/WUDC_manual.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting text check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the resulting text file\n",
    "with open(\"Texts/output2.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning using Microsoft's LoRA\n",
    "\n",
    "git clone https://github.com/microsoft/LoRA.git\n",
    "cd LoRA\n",
    "pip install -e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "with open('Texts/output2.txt', 'r') as f:\n",
    "    sentences = f.read().splitlines()\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_sentences, val_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "len(train_sentences), len(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer with a pretrained RoBERTa model\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "def preprocess(sentences):\n",
    "    # Tokenize the sentences\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence\n",
    "        #   (2) Prepend the `[CLS]` token to the start and append the `[SEP]` token to the end\n",
    "        #   (3) Map tokens to their IDs\n",
    "        #   (4) Create the attention mask\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Return a dictionary of outputs\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,                      # Sentence to encode\n",
    "            add_special_tokens=True,       # Add '[CLS]' and '[SEP]'\n",
    "            max_length=64,                 # Pad & truncate all sentences\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,    # Construct attention masks\n",
    "            return_tensors='pt',           # Return pytorch tensors\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding)\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Apply the preprocessing to the training and validation sentences\n",
    "train_input_ids, train_attention_masks = preprocess(train_sentences)\n",
    "val_input_ids, val_attention_masks = preprocess(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert the lists into datasets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset), # Random sampler for training\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset), # Sequential sampler for validation\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel\n",
    "from loralib import Embedding, Linear\n",
    "\n",
    "# Load the pretrained RoBERTa model\n",
    "config = RobertaConfig.from_pretrained('roberta-base')\n",
    "model = RobertaModel(config)\n",
    "\n",
    "# Replace the embedding layer with a LoRA embedding layer\n",
    "model.roberta.embeddings.word_embeddings = Embedding(\n",
    "    config.vocab_size, config.hidden_size, r=32, lora_alpha=1\n",
    ")\n",
    "\n",
    "# Replace the pooler layer with a LoRA linear layer\n",
    "model.roberta.pooler.dense = Linear(\n",
    "    config.hidden_size, config.hidden_size, r=32, lora_alpha=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_pdf_to_images(pdf_file_path, output_folder, base_name):\n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Convert the PDF to a list of images\n",
    "    images = convert_from_path(pdf_file_path)\n",
    "    \n",
    "    # Save each image to the output folder\n",
    "    for i, image in enumerate(images, start=1):\n",
    "        image.save(os.path.join(output_folder, f\"{base_name}_{i}.jpg\"), 'JPEG')\n",
    "\n",
    "# Use the function\n",
    "convert_pdf_to_images(\"PDFs/WUDC_manual.pdf\", \"JPEGs\", \"Manual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline\n",
    "document_answering_model = pipeline(task='question-answering', model='deepset/roberta-base-squad2')\n",
    "\n",
    "# Ask a question\n",
    "# Text defined above\n",
    "question = \"What is the meaning of life?\"\n",
    "answer = document_answering_model(question=question, context=text)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the type of motions and how do they work?\"\n",
    "answer = document_answering_model(question=question, context=text)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define the directory\n",
    "directory = \"JPEGs\"\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter for .jpg files and create full paths\n",
    "image_paths = [os.path.join(directory, file) for file in files if file.endswith(\".jpg\")]\n",
    "\n",
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the pipeline\n",
    "document_answering_model = pipeline(task='document-question-answering', model='impira/layoutlm-document-qa')\n",
    "\n",
    "# Iterate over the images, run OCR, and use the QA model\n",
    "for image_path in image_paths:\n",
    "    text = pytesseract.image_to_string(image_path)\n",
    "    question = \"What is the meaning of life?\"\n",
    "    answer = document_answering_model(question=question, context=text, image=image_path)\n",
    "\n",
    "    # Print the best answer if its score is above 0.75\n",
    "    if answer[0]['score'] > 0.75:\n",
    "        print(answer[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the images, run OCR, and use the QA model\n",
    "for image_path in image_paths:\n",
    "    text = pytesseract.image_to_string(image_path)\n",
    "    question = \"How does the WUDC debate format work?\"\n",
    "    answer = document_answering_model(question=question, context=text, image=image_path)\n",
    "\n",
    "    # Print the best answer if its score is above 0.75\n",
    "    if answer[0]['score'] > 0.75:\n",
    "        print(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_Verano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
